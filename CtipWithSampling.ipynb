{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols1 = ['star','rank','returnvalue','price_deduct','basic_minarea','basic_maxarea','roomtag_1','roomtag_2','roomtag_3','roomtag_4','roomtag_5','roomtag_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols1)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols1)\n",
    "#trdata.set_index('orderid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todata = pd.concat([trdata, tedata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scalar = preprocessing.MinMaxScaler()\n",
    "for dataset in combine:\n",
    "    dataset['saverate'] = dataset['returnvalue']/dataset['price_deduct']\n",
    "    dataset['returnvalue'] = min_max_scalar.fit_transform(dataset.returnvalue)\n",
    "    dataset['price_deduct'] = min_max_scalar.fit_transform(dataset.price_deduct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['basic_minarea'].fillna(dataset['basic_minarea'].median(),inplace=True)\n",
    "    dataset['basic_maxarea'].fillna(dataset['basic_maxarea'].median(),inplace=True)\n",
    "    dataset['basic_minarea'] = min_max_scalar.fit_transform(dataset.basic_minarea)\n",
    "    dataset['basic_maxarea'] = min_max_scalar.fit_transform(dataset.basic_maxarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['roomtag_2'].fillna(dataset['roomtag_2'].min(),inplace=True)\n",
    "    dataset['roomtag_3'].fillna(dataset['roomtag_3'].min(),inplace=True)\n",
    "    dataset['roomtag_1'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_4'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_5'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_6'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_2']=min_max_scalar.fit_transform(dataset['roomtag_2'])\n",
    "    dataset['roomtag_3']=min_max_scalar.fit_transform(dataset['roomtag_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for todata in combine:\n",
    "    todata['roomtag_total'] = todata['roomtag_1']+todata['roomtag_4']+todata['roomtag_5']+todata['roomtag_6']+todata['roomtag_3']+10*todata['roomtag_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tedata.isnull().any()\n",
    "trdata.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trdata_p1 = todata.loc[trdata.index]\n",
    "#tedata_p1 = todata.loc[tedata.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p1.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols2 = ['roomservice_1','roomservice_2','roomservice_3','roomservice_4','roomservice_5','roomservice_6','roomservice_7','roomservice_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols2)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols2)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['roomservice_total'] = 0\n",
    "    for roomservice in cols2:\n",
    "        dataset[roomservice].fillna(0, inplace=True)\n",
    "        dataset['roomservice_total'] += dataset[roomservice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p2.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#中途清理缓存\n",
    "del trdata\n",
    "del tedata\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trpart1 = pd.read_csv('./partfiles/trdata_p1.csv')\n",
    "trpart2 = pd.read_csv('./partfiles/trdata_p2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trtry = pd.merge(trpart1, trpart2, on = 'index')\n",
    "#trry = trpart1.join(trpart2)\n",
    "trtry = trpart1.concat(trpart2, axis=0)\n",
    "#继续释放缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols3 = ['user_confirmtime','user_avgadvanceddate','user_avgstar','user_avggoldstar','user_avgrecommendlevel','user_avgroomnum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols3)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols3)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:   \n",
    "    dataset['user_avgroomnum'].fillna(dataset['user_avgroomnum'].min(), inplace=True)\n",
    "    dataset['user_avgroomnum']=min_max_scalar.fit_transform(dataset['user_avgroomnum'])\n",
    "    for userinfo in cols3:\n",
    "        dataset[userinfo].fillna(dataset[userinfo].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p3.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols4 = ['ordertype_1_ratio','ordertype_2_ratio','ordertype_3_ratio','ordertype_4_ratio','ordertype_5_ratio','ordertype_6_ratio','ordertype_7_ratio','ordertype_8_ratio','ordertype_9_ratio','ordertype_10_ratio','ordertype_11_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols4)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols4)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in cols4:\n",
    "    trdata[col].fillna(0, inplace=True)\n",
    "    tedata[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p4.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols5 = ['user_avgdealpriceholiday','user_avgdealpriceworkday','user_avgdealprice','user_avgpromotion','user_avgprice_star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols5)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for useradv in cols5:\n",
    "    tedata[useradv].fillna(tedata[useradv].median(),inplace=True)\n",
    "    trdata[useradv].fillna(trdata[useradv].median(),inplace=True)\n",
    "    \n",
    "tedata['useradv_rate'] = tedata['user_avgpromotion']/tedata['user_avgdealprice']\n",
    "trdata['useradv_rate'] = trdata['user_avgpromotion']/trdata['user_avgdealprice']\n",
    "\n",
    "for useradv in cols5:\n",
    "    tedata[useradv]=min_max_scalar.fit_transform(tedata[useradv])\n",
    "    trdata[useradv]=min_max_scalar.fit_transform(trdata[useradv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p5.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols6 = ['orderbehavior_8','orderbehavior_9','orderbehavior_1_ratio','orderbehavior_2_ratio','orderbehavior_3_ratio_1week','orderbehavior_4_ratio_1week','orderbehavior_5_ratio_1week','orderbehavior_3_ratio_1month','orderbehavior_4_ratio_1month','orderbehavior_5_ratio_1month','orderbehavior_3_ratio_3month','orderbehavior_4_ratio_3month','orderbehavior_5_ratio_3month','orderbehavior_6_ratio','orderbehavior_7_ratio']\n",
    "aa = ['orderbehavior_8','orderbehavior_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols6)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols6)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    for col in aa:\n",
    "        dataset[col].fillna(dataset[col].median(), inplace=True)\n",
    "        dataset[col] = min_max_scalar.fit_transform(dataset[col])\n",
    "    for col in cols6:\n",
    "        dataset[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p6.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols7 = ['user_stdprice','user_cvprice','user_ordernum','user_activation','user_avgprice','user_maxprice','user_minprice','user_citynum','user_avgroomarea']\n",
    "aa = ['user_stdprice','user_cvprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols7)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols7)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    for userinfo2 in aa:\n",
    "        dataset[userinfo2].fillna(dataset[userinfo2].mean(), inplace=True)\n",
    "        dataset[userinfo2] = min_max_scalar.fit_transform(dataset[userinfo2])\n",
    "    for userinfo in cols7:\n",
    "        dataset[userinfo].fillna(dataset[userinfo].min(), inplace=True)\n",
    "        dataset[userinfo] = min_max_scalar.fit_transform(dataset[userinfo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p7.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols8 = ['user_roomservice_4_0ratio','user_roomservice_4_2ratio','user_roomservice_4_3ratio','user_roomservice_4_4ratio','user_roomservice_4_1ratio','user_roomservice_4_5ratio','user_roomservice_3_123ratio','user_roomservice_6_2ratio','user_roomservice_6_1ratio','user_roomservice_6_0ratio','user_roomservice_5_1ratio','user_roomservice_7_0ratio','user_roomservice_2_1ratio','user_roomservice_8_1ratio','user_rank_ratio','user_roomservice_5_345ratio','user_ordnum_1week','user_avgprice_1week','user_medprice_1week','user_minprice_1week','user_maxprice_1week','user_roomservice_3_123ratio_1week','user_roomservice_7_1ratio_1week','user_roomservice_7_0ratio_1week','user_roomservice_4_5ratio_1week','user_roomservice_4_4ratio_1week','user_roomservice_4_2ratio_1week','user_roomservice_4_3ratio_1week','user_roomservice_4_0ratio_1week','user_ordnum_1month','user_avgprice_1month','user_medprice_1month','user_minprice_1month','user_maxprice_1month','user_roomservice_3_123ratio_1month','user_roomservice_7_1ratio_1month','user_roomservice_7_0ratio_1month','user_roomservice_4_5ratio_1month','user_roomservice_4_4ratio_1month','user_roomservice_4_2ratio_1month','user_roomservice_4_3ratio_1month','user_roomservice_4_0ratio_1month','user_ordnum_3month','user_avgprice_3month','user_medprice_3month','user_minprice_3month','user_maxprice_3month','user_roomservice_3_123ratio_3month','user_roomservice_7_1ratio_3month','user_roomservice_7_0ratio_3month','user_roomservice_4_5ratio_3month','user_roomservice_4_4ratio_3month','user_roomservice_4_2ratio_3month','user_roomservice_4_3ratio_3month','user_roomservice_4_0ratio_3month','basic_week_ordernum_ratio','basic_recent3_ordernum_ratio','basic_comment_ratio','basic_30days_ordnumratio','basic_30days_realratio','room_30days_ordnumratio','room_30days_realratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols8)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols8)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    for rest in cols8:\n",
    "        dataset[rest].fillna(dataset[rest].min(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p8.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols9 = ['rank_lastord','return_lastord','price_last_lastord','star_lastord']\n",
    "# 'hotel_minprice_lastord','basic_minprice_lastord' 被丢弃 然而事实证明这些反而是很有用的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols9)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols9)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['rank_lastord'].fillna(-1, inplace=True)\n",
    "    dataset['star_lastord'].fillna(-1, inplace=True)\n",
    "    dataset['return_lastord'].fillna(dataset['return_lastord'].min(), inplace=True)\n",
    "    dataset['price_last_lastord'].fillna(dataset['price_last_lastord'].min(), inplace=True)\n",
    "    dataset['priceratel'] = dataset['return_lastord']/(dataset['price_last_lastord']+1)\n",
    "        \n",
    "    dataset['priceratel'].fillna(0, inplace=True)\n",
    "    \n",
    "#    for col in cols9:\n",
    "#        dataset[col].fillna(dataset[col].min, inplace=True)\n",
    "        \n",
    "    #这里注意，分母不加1会有为0的情况\n",
    "    #   for col in cols9:\n",
    "    #       dataset[col] = min_max_scalar.fit_transform(dataset[col])\n",
    "    #缺失值填成-1便不能做标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for dataset in combine:\n",
    "#    dataset = dataset.drop(['hotel_minprice_lastord','basic_minprice_lastord'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p9.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols10 = ['roomservice_2_lastord','roomservice_3_lastord','roomservice_4_lastord','roomservice_5_lastord','roomservice_6_lastord','roomservice_8_lastord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols10)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols10)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    for roomservice in cols10:   \n",
    "        dataset[roomservice].fillna(0, inplace=True)\n",
    "\n",
    "    dataset['roomslt'] = 0\n",
    "    for roomservice in cols10:\n",
    "        dataset['roomslt'] += dataset[roomservice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p10.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols11 = ['roomtag_2_lastord','roomtag_3_lastord','roomtag_4_lastord','roomtag_5_lastord','roomtag_6_lastord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata = pd.read_csv('competition_train.txt', sep='\\t', usecols = cols11)\n",
    "tedata = pd.read_csv('competition_test.txt', sep='\\t', usecols = cols11)\n",
    "combine = [trdata, tedata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['roomtag_2_lastord'].fillna(dataset['roomtag_2_lastord'].min(),inplace=True)\n",
    "    dataset['roomtag_3_lastord'].fillna(dataset['roomtag_3_lastord'].min(),inplace=True)\n",
    "    dataset['roomtag_2_lastord']=min_max_scalar.fit_transform(dataset['roomtag_2_lastord'])\n",
    "    dataset['roomtag_3_lastord']=min_max_scalar.fit_transform(dataset['roomtag_3_lastord'])\n",
    "    dataset['roomtag_4_lastord'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_5_lastord'].fillna(0, inplace=True)\n",
    "    dataset['roomtag_6_lastord'].fillna(0, inplace=True)\n",
    "\n",
    "    dataset['roomtag_total_last'] = dataset['roomtag_4_lastord']+dataset['roomtag_5_lastord']+dataset['roomtag_6_lastord']+dataset['roomtag_3_lastord']+10*dataset['roomtag_2_lastord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trdata.to_csv('./partfiles/trdata_p11.csv', index=False)\n",
    "tedata.to_csv('./partfiles/tedata_p11.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，要用的数据处理完毕，下面进行整合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trpart1 = pd.read_csv('./partfiles/trdata_p1.csv')\n",
    "trpart2 = pd.read_csv('./partfiles/trdata_p2.csv')\n",
    "trpart3 = pd.read_csv('./partfiles/trdata_p3.csv')\n",
    "trpart4 = pd.read_csv('./partfiles/trdata_p4.csv')\n",
    "trpart5 = pd.read_csv('./partfiles/trdata_p5.csv')\n",
    "trpart6 = pd.read_csv('./partfiles/trdata_p6.csv')\n",
    "trpart7 = pd.read_csv('./partfiles/trdata_p7.csv')\n",
    "trpart8 = pd.read_csv('./partfiles/trdata_p8.csv')\n",
    "trpart9 = pd.read_csv('./partfiles/trdata_p9.csv')\n",
    "trpart10 = pd.read_csv('./partfiles/trdata_p10.csv')\n",
    "trpart11 = pd.read_csv('./partfiles/trdata_p11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_total = trpart1.join(trpart2).join(trpart3).join(trpart4).join(trpart5).join(trpart6).join(trpart7).join(trpart8).join(trpart9).join(trpart10).join(trpart11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#需要添加释放缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_total.to_csv('./train_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tepart1 = pd.read_csv('./partfiles/tedata_p1.csv')\n",
    "tepart2 = pd.read_csv('./partfiles/tedata_p2.csv')\n",
    "tepart3 = pd.read_csv('./partfiles/tedata_p3.csv')\n",
    "tepart4 = pd.read_csv('./partfiles/tedata_p4.csv')\n",
    "tepart5 = pd.read_csv('./partfiles/tedata_p5.csv')\n",
    "tepart6 = pd.read_csv('./partfiles/tedata_p6.csv')\n",
    "tepart7 = pd.read_csv('./partfiles/tedata_p7.csv')\n",
    "tepart8 = pd.read_csv('./partfiles/tedata_p8.csv')\n",
    "tepart9 = pd.read_csv('./partfiles/tedata_p9.csv')\n",
    "tepart10 = pd.read_csv('./partfiles/tedata_p10.csv')\n",
    "tepart11 = pd.read_csv('./partfiles/tedata_p11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_total = tepart1.join(tepart2).join(tepart3).join(tepart4).join(tepart5).join(tepart6).join(tepart7).join(tepart8).join(tepart9).join(tepart10).join(tepart11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_total.to_csv('./test_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理完成，下面跑一个默认参数的算法，看看结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train_total.csv')\n",
    "la = ['orderlabel']\n",
    "y_train = pd.read_csv('competition_train.txt', sep='\\t', usecols = la)\n",
    "\n",
    "label = ['orderid','roomid']\n",
    "data = pd.read_csv('competition_test.txt', sep='\\t', usecols = label)\n",
    "X_test = pd.read_csv('test_total.csv')\n",
    "X_test['orderid'] =  data['orderid']\n",
    "X_test['roomid'] =  data['roomid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1 = X_test[:1500000]\n",
    "X_test2 = X_test[1500000:3000000]\n",
    "X_test3 = X_test[3000000:4500000]\n",
    "X_test4 = X_test[4500000:6000000]\n",
    "X_test5 = X_test[6000000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test1.to_csv('./partsample/X_test_p1.csv', index=False)\n",
    "X_test2.to_csv('./partsample/X_test_p2.csv', index=False)\n",
    "X_test3.to_csv('./partsample/X_test_p3.csv', index=False)\n",
    "X_test4.to_csv('./partsample/X_test_p4.csv', index=False)\n",
    "X_test5.to_csv('./partsample/X_test_p5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尴尬，全数据还是跑不了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['orderlabel']=y_train['orderlabel']\n",
    "X_train.to_csv('train_total1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pos = X_train.loc[X_train['orderlabel']==1]\n",
    "X_train_neg = X_train.loc[X_train['orderlabel']==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#实行正负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=7513295\n",
    "X_train_neg_p1 = X_train_neg[:int(a/10)]\n",
    "X_train_neg_p2 = X_train_neg[round(a/10):int(2*a/10)]\n",
    "X_train_neg_p3 = X_train_neg[round(2*a/10):int(3*a/10)]\n",
    "X_train_neg_p4 = X_train_neg[round(3*a/10):int(4*a/10)]\n",
    "X_train_neg_p5 = X_train_neg[round(4*a/10):int(5*a/10)]\n",
    "X_train_neg_p6 = X_train_neg[round(5*a/10):int(6*a/10)]\n",
    "X_train_neg_p7 = X_train_neg[round(6*a/10):int(7*a/10)]\n",
    "X_train_neg_p8 = X_train_neg[round(7*a/10):int(8*a/10)]\n",
    "X_train_neg_p9 = X_train_neg[round(8*a/10):int(9*a/10)]\n",
    "X_train_neg_p10 = X_train_neg[round(9*a/10):]\n",
    "#这边有误，应该全部取int\n",
    "len(X_train_neg_p1)+len(X_train_neg_p2)+len(X_train_neg_p3)+len(X_train_neg_p4)+len(X_train_neg_p5)+len(X_train_neg_p6)+len(X_train_neg_p7)+len(X_train_neg_p8)+len(X_train_neg_p9)+len(X_train_neg_p10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分别构造10个分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 完全可以用for 循环代替\n",
    "num = range(10)\n",
    "for i in num:\n",
    "    X = eval('X_train_p'+str(i+1))\n",
    "    X_neg = eval('X_train_neg_p'+str(i+1))\n",
    "    X = pd.concat([X_train_pos,X_neg])\n",
    "    X.tp_csv('./partsample/X_train_p'+str(i+1)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p1 = pd.concat([X_train_pos, X_train_neg_p1])\n",
    "X_train_p1.to_csv('./partsample/X_train_p1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p2 = pd.concat([X_train_pos, X_train_neg_p2])\n",
    "X_train_p2.to_csv('./partsample/X_train_p2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p3 = pd.concat([X_train_pos, X_train_neg_p3])\n",
    "X_train_p3.to_csv('./partsample/X_train_p3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p4 = pd.concat([X_train_pos, X_train_neg_p4])\n",
    "X_train_p4.to_csv('./partsample/X_train_p4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p5 = pd.concat([X_train_pos, X_train_neg_p5])\n",
    "X_train_p5.to_csv('./partsample/X_train_p5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p6 = pd.concat([X_train_pos, X_train_neg_p6])\n",
    "X_train_p6.to_csv('./partsample/X_train_p6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p7 = pd.concat([X_train_pos, X_train_neg_p7])\n",
    "X_train_p7.to_csv('./partsample/X_train_p7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p8 = pd.concat([X_train_pos, X_train_neg_p8])\n",
    "X_train_p8.to_csv('./partsample/X_train_p8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p9 = pd.concat([X_train_pos, X_train_neg_p9])\n",
    "X_train_p9.to_csv('./partsample/X_train_p9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p10 = pd.concat([X_train_pos, X_train_neg_p10])\n",
    "X_train_p10.to_csv('./partsample/X_train_p10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复10次，把训练数据分批处理进文件，不然读入test的时候会内存爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_p10 = pd.read_csv('./partsample/X_train_p10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train10 = X_train_p10.drop(['orderlabel'],axis=1)\n",
    "y_train10 = X_train_p10['orderlabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "xgbc0 = XGBClassifier()\n",
    "xgbc10.fit(X_train10, y_train10)\n",
    "\n",
    "print (cross_val_score(xgbc2, X_train2, y_train2, cv=5, scoring='roc_auc').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pro1 = xgbc1.predict_proba(X_test1)\n",
    "y_pro6 = xgbc6.predict_proba(X_test1)\n",
    "\n",
    "final6=[]\n",
    "for i in y_pro6:\n",
    "    final6.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getpro(X_test, clf):\n",
    "    y_pro = clf.predict_proba(X_test)\n",
    "    fin=[]\n",
    "    for i in y_pro:\n",
    "        fin.append(i[1])\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这边可以直接 用 y_pro = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final10 = getpro(X_test1, xgbc10)\n",
    "final22 = getpro(X_test2, xgbc2)\n",
    "final31 = getpro(X_test3, xgbc1)\n",
    "final41 = getpro(X_test4, xgbc1)\n",
    "final51 = getpro(X_test5, xgbc1)\n",
    "final510 = getpro(X_test5, xgbc10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub5 = pd.DataFrame({'orderid':X_test_p5['orderid'], 'predict_roomid':X_test_p5['roomid'], 'f1':final51, 'f2':final52, 'f3':final53, 'f4':final54, 'f5':final55,'f6':final56, 'f7':final57, 'f8':final58, 'f9':final59, 'f10':final510})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftotal = ['f1','f2','f3','f4','f5','f6','f7','f8','f9','f10']\n",
    "sub5['ftotal'] = 0\n",
    "\n",
    "for i in ftotal:\n",
    "    sub5['ftotal'] += sub5[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这边得到10个结果其实可以生成一波stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub5 = sub5[['orderid', 'predict_roomid', 'ftotal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub5.to_csv('./result/re_p5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsub = [sub1,sub2,sub33,sub4,sub5]\n",
    "subfinal = pd.concat(subsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsub2 = subfinal.sort_index(by=['orderid', 'ftotal'],ascending=False)\n",
    "sub_xgbc_final = subsub2.groupby(subsub2['orderid']).head(1)\n",
    "sub_xgbc_final = sub_xgbc_final.drop(['ftotal'], axis=1)\n",
    "sub_xgbc_final.to_csv('./sub_xgbc4sampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练都训练不了，单独把模型保存起来看看能不能训练Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(xgbc10, './models/xgbc10.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgbc1 = joblib.load('./models/xgbc1.m')\n",
    "xgbc2 = joblib.load('./models/xgbc2.m')\n",
    "xgbc3 = joblib.load('./models/xgbc3.m')\n",
    "xgbc4 = joblib.load('./models/xgbc4.m')\n",
    "xgbc5 = joblib.load('./models/xgbc5.m')\n",
    "xgbc6 = joblib.load('./models/xgbc6.m')\n",
    "xgbc7 = joblib.load('./models/xgbc7.m')\n",
    "xgbc8 = joblib.load('./models/xgbc8.m')\n",
    "xgbc9 = joblib.load('./models/xgbc9.m')\n",
    "xgbc10 = joblib.load('./models/xgbc10.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_p5 = pd.read_csv('./partsample/X_test_p5.csv')\n",
    "X_test5 = X_test_p5.drop(['orderid','roomid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_xgbc = pd.DataFrame({'orderid':data['orderid'],'predict_roomid':data['roomid'],'predict':final})\n",
    "subsub = sub_xgbc.sort_index(by=['orderid', 'predict'], ascending=False)\n",
    "sub_xgbc_final = subsub.groupby(subsub['orderid']).head(1)\n",
    "sub_xgbc_final = sub_xgbc_final.drop(['predict'],axis=1)\n",
    "sub_xgbc_final.to_csv('./new_xgbc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_imp1 = pd.Series(xgbc1.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp2 = pd.Series(xgbc2.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp3 = pd.Series(xgbc3.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp4 = pd.Series(xgbc4.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp5 = pd.Series(xgbc5.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp6 = pd.Series(xgbc6.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp7 = pd.Series(xgbc7.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp8 = pd.Series(xgbc8.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp9 = pd.Series(xgbc9.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp10 = pd.Series(xgbc10.booster().get_fscore()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_imps = [feat_imp1,feat_imp2,feat_imp3,feat_imp4,feat_imp5,feat_imp6,feat_imp7,feat_imp8,feat_imp9,feat_imp10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feat_imp in feat_imps:\n",
    "    feat_imp = feat_imp.loc[feat_imp >4]\n",
    "    \n",
    "total_feat = pd.concat(feat_imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train_total1.csv', usecols = feat_imp)\n",
    "y_train = pd.read_csv('train_total1.csv', usecols = ['orderlabel'])\n",
    "y = y_train['orderlabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(xgbc0, './models/xgbc0.m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = ['orderid','roomid']\n",
    "data = pd.read_csv('competition_test.txt', sep='\\t', usecols = label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
